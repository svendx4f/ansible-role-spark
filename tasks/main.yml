---
- name: install spark dependancy for Debian OS family
  apt: pkg=git
  environment: spark_environment
  when: ansible_os_family == 'Debian'
  tags: ["packages","spark"]

- name: ensure spark group exist
  group: name={{spark_group}}
  tags: ["packages","spark"]

- name: ensure spark user exist
  user: name={{spark_user}} group={{spark_group}}
  tags: ["packages","spark"]

- name: ensure spark install dir exist and belong to spark user
  file: state=directory path={{spark_install_dir}} owner={{spark_user}} group={{spark_group}} recurse=yes
  tags: ["packages","spark"]



- name: retrieve spark from git
  sudo_user: "{{spark_user}}"
  tags: packerio
  git: > 
     repo={{spark_repository}} dest={{spark_install_dir}} 
     version={{spark_version}} update=yes depth=1 
  environment: spark_environment
  tags: ["packages","spark"]
  when: spark_binary_provider.method == "build"

- name: checks if spark compilation is required (if method=build)
  stat: 
    path="{{spark_binary_provider.produces}}"
  register: spark_assembly_file
  when: spark_binary_provider.method == "build" and spark_binary_provider.produces is defined

- name: spark compilation (if method=build and binary not present yet)
  sudo_user: "{{spark_user}}"
  shell: "{{item}}"
  args:
    chdir: "{{spark_install_dir}}"
  environment: spark_environment
  with_items:
     - mvn dependency:resolve
#     - sbt/sbt assembly {{spark_compile_options}}
     - mvn clean package {{spark_compile_options}}
  tags: ["compilation","spark"]
  when: spark_binary_provider.method == 'build' and (spark_binary_provider.produces is undefined or not spark_assembly_file.stat.exists)


- name: downloads spark bin from s3 (if repository is s3)
  s3: 
    bucket="{{spark_binary_provider.bucket}}"
    object="{{spark_binary_provider.s3_path}}/{{spark_binary_provider.package_name}}"
    dest="{{spark_install_dir}}/{{spark_binary_provider.package_name}}"
    region="{{spark_binary_provider.region}}"
    mode=get
  environment: aws_cred_env
  when: spark_binary_provider.method == 'download' and spark_binary_provider.repository == 's3'

- name: uncompress spark s3 package
  unarchive: 
    src="{{spark_install_dir}}/{{spark_binary_provider.package_name}}"  
    dest="{{spark_install_dir}}/.."
    owner="{{spark_user}}"
    group="{{spark_group}}"
    copy=no
  when: spark_binary_provider.method == 'download'


- name: configure spark
  template: src={{item}}.j2 dest={{spark_install_dir}}/conf/{{item}} owner={{spark_user}}
  with_items:
     - spark-defaults.conf
     - spark-env.sh
  tags: ["configuration","spark"]

- name: add symlink for default spark version
  file: state=link src={{spark_install_dir}} dest={{spark_link_dir}}
  tags: ["configuration","spark"]

- name: configure spark env for user
  template: src=spark.sh.j2 dest=/etc/profile.d/spark.sh
  tags: ["configuration","spark"]

